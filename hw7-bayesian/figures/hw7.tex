
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{hw7}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    \section{Coin Flipping: Maximum
Likelihood}\label{coin-flipping-maximum-likelihood}

    \subsection{Question 1}\label{question-1}

Suppose we flip a coin and get the following seqeunce of heads and
tails: \[\mathcal D = (H, H, T)\] Give an expression for the probability
of observing \(\mathcal D\) given that the probability of heads is
\(\theta\). That is, give an expression for \(p(D | \theta)\).
\textbf{This is called the likelihood of \(\theta\) for the data \(D\).}

    \begin{quote}
\textbf{ANSWER}
\end{quote}

\begin{quote}
\(p(D|\theta)=\theta*\theta*(1-\theta)\)
\end{quote}

\begin{quote}
\(= \theta^2-\theta^3\)
\end{quote}

    \subsection{Question 2}\label{question-2}

How many different sequences of 3 coin tosses have 2 heads and 1 tail?
If we coss the coin 3 times, what is the probability of 2 heads and 1
tail? (Answer should be in terms of \(\theta\).)

    \begin{quote}
\textbf{ANSWER}
\end{quote}

\begin{quote}
There are three different sequences of 2 heads and 1 tail. \{HHT,
HTH,THH\}
\end{quote}

\begin{quote}
The probability to get 2 heads and 1 tail is \(3\theta^2-3\theta^3\)
\end{quote}

    \subsection{Question 3}\label{question-3}

More generally, give an expression for the likelihood \(p(D|\theta)\)
for a particular sequence of flips D that has \(n_h\) heads and \(n_t\)
tails. Make sure you have expressions that make sense even for
\(\theta=0\) and \(n_h=0\), and other boundary cases. You may use the
convention that \(0^0 = 1\), or you can break your expression into cases
if needed.

    \begin{quote}
\textbf{ANSWER}
\end{quote}

\begin{quote}
\(p(D|\theta)=\theta^{n_h}(1-\theta)^{n_t}\)
\end{quote}

    \subsection{Question 4}\label{question-4}

Prove that the maximum likelihood estimate of \(\theta\) given we
observed a sequence with \(n_h\) heads and \(n_t\) tails is
\[\hat{\theta}_{\text{MLE}} =\frac{n_h}{n_h+n_t}.\]

    \begin{quote}
\textbf{ANSWER}
\end{quote}

\begin{quote}
\(log(p(D|\theta)) = n_hlog\theta+n_tlog(1-\theta)\)
\end{quote}

\begin{quote}
for which we take the derivative,
\end{quote}

\begin{quote}
\(\frac{\partial log(p(D|\theta))}{\partial \theta}=\frac{n_h}{\theta}-\frac{n_t}{1-\theta}=0\)
\end{quote}

\begin{quote}
\(\theta = \frac{n_h}{n_h+n_t}\)
\end{quote}

\clearpage

    \section{Coin Flipping: Bayesian Approach with Beta
Prior}\label{coin-flipping-bayesian-approach-with-beta-prior}

We'll now take a Bayesian approach to the coin flipping problem, in
which we treat \(\theta\) as a random variable sampled from some prior
distribution \(p(\theta)\). We'll represent the ith coin flip by a
random variable \(X_i\in{0,1}\), where \(X_i=1\) if the \(i\)th flip is
heads. We assume that the \(X_i’s\) are conditionally indpendent given
\(\theta\). This means that the joint distribution of the coin flips and
\(\theta\) factorizes as follows:
\[p(x_1,\cdots, x_n,\theta) = p(\theta)p(x_1,\cdots, x_n|\theta) \text{ (always true)}\]
\[= p(\theta)\Pi_{i=1}^n p(x_i| θ) \text{ (by conditional independence)}.\]

    \subsection{Question 1}\label{question-1}

Suppose that our prior distribution on \(\theta\) is \(Beta(h, t)\), for
some \(h,t>0\). That is,
\(p(\theta)\propto\theta^{h−1}(1-\theta)^{t−1}\). Suppose that our
sequence of flips D has \(n_h\) heads and \(n_t\) tails. Show that the
posterior distribution for \(\theta\) is \(Beta(h+n_h,t+n_t)\). That is,
show that \[p(\theta|D)\propto\theta^{h−1+n_h}(1-\theta)^{t−1+n_t}.\] We
say that the Beta distribution is \textbf{conjugate} to the Bernoulli
distribution since the prior and the posterior are both in the same
family of distributions (i.e. both Beta distributions).

    \begin{quote}
\textbf{ANSWER}
\end{quote}

\begin{quote}
\(p(\theta|D)\propto p(\theta)p(D|\theta)\)
\end{quote}

\begin{quote}
\(=\theta^{h-1}(1-\theta)^{t-1}\times\theta^{n_h}(1-\theta)^{n_t}\)
\end{quote}

\begin{quote}
\(=\theta^{h-1+n_h}(1-\theta)^{t-1+n_t}.\)
\end{quote}

    \subsection{Question 2}\label{question-2}

Give expressions for the MLE, the MAP, and the posterior mean estimates
of \(\theta\). {[}Hint: You may use the fact that a Beta(h, t)
distribution has mean h/(h + t) and has mode (h − 1) / (h + t − 2) for
h, t \textgreater{} 1. For the Bayesian solutions, you should note that
as h + t gets very large, the posterior mean and MAP approach the prior
mean h/ (n + h), while for fixed h and t, the posterior mean approaches
the MLE when \(n_h+n_t\to\infty\).

    \begin{quote}
\textbf{ANSWER}
\end{quote}

\begin{quote}
\(\hat\theta_{MLE}=argmin_{\theta}log(p(\theta|D))=\frac{n_h}{n_h+n_t}\)
\end{quote}

\begin{quote}
\(\hat\theta_{MAP}=argmax_{\theta}p(\theta|D)=\frac{h + n_h - 1}{h + n_h + t + n_t - 2}\)
\end{quote}

\begin{quote}
\(\hat\theta_{POSTERIOR}=E(\theta|D)=\frac{h+n_h}{h+t+n_h+n_t}\)
\end{quote}

    \subsection{Question 3}\label{question-3}

What happens to \(\hat\theta_{MLE}\),\(\hat\theta_{MAP}\), and
\(\hat\theta_{POSTERIOR MEAN}\) as the number of coin flips approaches
infinity?

    \begin{quote}
\textbf{ANSWER}
\end{quote}

\begin{quote}
As the number of coin flips approaches infinity, all of them converge to \(\theta\). As we get more data the effect of prior reduces.
\end{quote}

    \subsection{Question 4}\label{question-4}

The MAP and posterior mean estimators of \(\theta\) were derived from a
Bayesian perspective. Let's now evaluate them from a frequentist
perspective. Suppose \(\theta\) is fixed and unknown. Which of the MLE,
MAP, and posterior mean estimators give unbiased estimates of θ, if any?
{[}Hint:The answer may depend on the parameters h and t of the prior.{]}

\begin{quote}
\textbf{ANSWER}
\end{quote}

\begin{quote}

The MLE is unbiased in this case. In general, the others are biased. The
posterior mean is asymptotically unbiased as h,t approaches 0. The MAP is unbiased unless h = t = 1.
    \subsection{Question 5}\label{question-5}

Suppose somebody gives you a coin and asks you to give an estimate of
the probability of heads, but you can only toss the coin 3 times. You
have no particular reason to believe this is an unfair coin. Would you
prefer the MLE or the posterior mean as a point estimate of \(\theta\)?
If the posterior mean, what would you use for your prior?
\end{quote}

    \begin{quote}
\textbf{ANSWER}
\end{quote}

\begin{quote}
I would use posterior mean, and the prior I will use will be the beta
distribution.
\end{quote}

\clearpage


    \section{Hierarchical Bayes for Click-Through Rate
Estimation}\label{hierarchical-bayes-for-click-through-rate-estimation}

    \subsection{Empirical Bayes for a single
app}\label{empirical-bayes-for-a-single-app}

We start by working out some details for Bayesian inference for a single
app. That is, suppose we only have the data \(D_i\) from app \(i\), and
nothing else. Mathematically, this is exactly the same setting as the
coin tossing setting above, but here we push it further.

    1 Give an expression for \(p(D_i|\theta_i)\), the likelihood of \(D_i\)
given the probability of click \(\theta_i\), in terms of \(\theta_i\),
\(x_i\) and \(n_i\).

    \begin{quote}
\textbf{ANSWER}
\end{quote}

\begin{quote}
\(p(D_i|\theta_i)=\theta_i^{x_i}(1-\theta_i)^{n_i-x_i}\)
\end{quote}

    2 The probability density for the \(Beta(a,b)\) distribution, evaluated
at \(\theta_i\), is given by
\[Beta(\theta_i;a,b)=\frac{1}{B(a,b)}\theta_i^{\alpha-1}(1-\theta_i)^{b−1}\]

where \(B(a,b)\) is called the Beta function. Explain why we must have
\(\int\theta^{a−1}_i(1-\theta_i)^{b−1}d\theta=B(a, b)\), and give the
full density function for the prior on \(\theta_i\), in terms of
\(a,b\), and the normalization function B.

    \begin{quote}
\textbf{ANSWER}
\end{quote}

\begin{quote}
We need the total probability integrals to 1, so we must have
\(\int\theta^{a−1}_i(1-\theta_i)^{b−1}d\theta=B(a, b)\)
\end{quote}

\begin{quote}
The full density function for \(\theta_i\) is
\[Beta(\theta_i;a,b)=\frac{1}{B(a,b)}\theta_i^{\alpha-1}(1-\theta_i)^{b−1}\]
\end{quote}

    3 Give an expression for the posterior distribution \(p(\theta_i|D_i)\).
In this case, include the constant of proportionality. (In other words,
do not use the ``is proportional to'' sign \(\propto\) in your final
expression.) {[}Hint: This problem is essentially a repetition of an
earlier problem.{]}

\begin{quote}
\textbf{ANSWER}
\end{quote}

\begin{quote}
\(p(\theta_i|D_i)=c p(\theta_i)p(D_i|\theta_i)\)
\end{quote}

\begin{quote}
\(=\frac{c}{B(a,b)}\theta_i^{a-1+x_i}(1-\theta_i)^{b-1+n_i-x_i}\)
\end{quote}

4 Give a closed form expression for \(p(D_i)\), the marginal likelihood
of \(D_i\), in terms of the \(a\), \(b\), \(x_i\), and \(n_i\). You may
use the normalization function \(B(·, ·)\) for convenience, but you
should not have any integrals in your solution. (Hint:
\(p(D_i) = p (D_i|\theta_i)p(\theta_i)d\theta_i\), and the answer will
be a ratio of two beta function evaluations.)

\begin{quote}
\textbf{ANSWER}
\end{quote}

\begin{quote}
\(p(D_i)=\int p(D_i|\theta_i)p(\theta_i)d\theta(i)\)
\end{quote}

\begin{quote}
\(=\frac{1}{B(a,b)}\theta_i^{a-1}(1-\theta_i)^{b-1}\theta^{x_i}(1-\theta)^{n_i-x_i}\)
\end{quote}

\begin{quote}
\(=\frac{1}{B(a,b)}\theta_i^{a-1+x_i}(1-\theta_i)^{b-1+n_i-x_i}\)
\end{quote}

\begin{quote}
\(=\frac{B(a-1+x_i, b-1+n_i-x_i)}{B(a,b)}\)
\end{quote}

    5 The maximum likelihood estimate for \(\theta_i\) is \(x_i/n_i\). Let
\(p_{MLE}(D_i)\) be the marginal likelihood of \(D_i\) when we use a
prior on \(\theta_i\) that puts all of its probability mass at
\(x_i/n_i\)d. Note that
\(p_{MLE}(D_i) = p(D_i|\theta_i =\frac{x_i}{n_i}p(\theta_i =\frac{x_i}{n_i}= p(D_i|\theta_i) =\frac{x_i}{n_i}\).
Explain why, or prove, that \(p_{MLE}(D_i)\) is larger than \(p(D_i)\)
for any other prior we might put on \(\theta_i\). If it's too hard to
reason about all possible priors, it's fine to just consider all Beta
priors.

    \begin{quote}
\textbf{ANSWER}
\end{quote}

\begin{quote}
\(p(D_i)=\int p(D_i|\theta_i)p(\theta_i)d\theta_i\)
\end{quote}

\begin{quote}
\(\approx \sum p(D_i|\theta_i)p(\theta_i)\)
\end{quote}

\begin{quote}
\(<\sum p(D_i|\theta_i=\frac{x_i}{n_i})p(\theta_i)\)
\end{quote}

\begin{quote}
\(<p_{MLE}D_i\)
\end{quote}

    6 One approach to getting an empirical Bayes estimate of the parameters
a and b is to use maximum likelihood. Such an empirical Bayes estimate
is often called an ML-2 estimate, since it's maximum likelihood, but at
a higher level in the Bayesian hierarchy. To emphasize the dependence of
the likelihood of \(D_i\) on the parameters a and b, we'll now write it
as \(p(D_i| a, b)\). The empirical Bayes estimates for a and b are given
by

\[(\hat a, \hat b) = arg\max\limits_{(a,b)∈R>0×R>0} p(D_i | a, b).\]

To make things concrete, suppose we observed \(x_i = 3\) clicks out of
\(n_i = 500\) impressions. A plot of \(p(D_i| a, b)\) as a function of a
and b is given in Figure 1. It appears from this plot that the
likelihood will keep increasing as a and b increase, at least if a and b
maintain a particular ratio. Indeed, this likelihood function never
attains its maximum, so we cannot use ML-2 here. Explain what's
happening to the prior as we continue to increase the likelihood.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} 
\end{Verbatim}

\clearpage

    \subsection{Empirical Bayes Using All App
Data}\label{empirical-bayes-using-all-app-data}

In the previous section, we considered working with data from a single
app. With a fixed prior, such as Beta(3,400), our Bayesian estimates for
\(\theta_i\) seem more reasonable (to me, the person who chose the
prior) than the MLE when our sample size \(n_i\) is small. The fact that
these estimates seem reasonable is an immediate consequence of the fact
that I chose the prior to give high probability to estimates that seem
reasonable to me, before ever seeing the data. Our attempt to use
empirical Bayes (ML-2) to choose the prior in a data-driven way was not
successful. With only a single app, we were essentially overfitting the
prior to the data we have. In this section, we'll consider using the
data from all the apps, in which case empirical Bayes makes more sense.

    1 Let \(D=(D_1, \cdots , D_d)\) be the data from all the apps. Give an
expression for \(p(D | a, b)\), the marginal likelihood of D. Expression
should be in terms of \(a, b, x_i, n_i\) for \(i = 1, \cdots, d\).
(Hint: This problem should be easy, based on a problem from the previous
section.)

    \begin{quote}
\textbf{ANSWER}
\end{quote}

\begin{quote}
\(p(D|a,b)=\frac{B(a-1+x_i, b-1+n_i-x_i)}{B(a,b)}\)
\end{quote}

    2 Explain why \(p(\theta_i| D) = p(\theta_i| Di)\), according to our
model. In other words, once we choose values for parameters a and b,
information about one app does not give any information about other
apps.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} 
\end{Verbatim}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
